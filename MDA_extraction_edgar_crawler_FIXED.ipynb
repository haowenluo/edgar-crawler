{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIXED: MD&A EXTRACTION WITH YEAR SUBFOLDER ORGANIZATION\n",
    "\n",
    "**Purpose:** Extract MD&A sections from 10-K filings with Drive timeout fix\n",
    "\n",
    "**CRITICAL FIX:** Reorganizes files into year subfolders to avoid Google Drive's ~10,000 file limit\n",
    "\n",
    "**Prerequisites:**\n",
    "- Repository: `/content/drive/MyDrive/EDGAR_Project/edgar-crawler`\n",
    "- Raw 10-K files downloaded\n",
    "- **~75,000 extracted files causing Drive timeouts** ‚Üê This notebook fixes that!\n",
    "\n",
    "**Instructions:**\n",
    "- üü¢ **GREEN** = Run EVERY TIME\n",
    "- üü° **YELLOW** = Run FIRST TIME ONLY\n",
    "- üîµ **BLUE** = Optional/conditional\n",
    "- üî¥ **RED** = CRITICAL FIX - Run once to reorganize files\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 1: Mount Google Drive\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"‚úÖ Drive already mounted\")\n",
    "else:\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 2: Navigate to Repository\n",
    "import os\n",
    "\n",
    "REPO_DIR = '/content/drive/MyDrive/EDGAR_Project/edgar-crawler'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"‚ùå Repository not found at: {REPO_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 3: Install Dependencies\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "!pip install -q 'dill<0.3.9' 'multiprocess<0.70.17'\n",
    "!pip install -q pox ppft\n",
    "!pip install -q --no-deps pathos\n",
    "!pip install -q beautifulsoup4 lxml requests pandas tqdm click cssutils numpy pyarrow\n",
    "\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 4: Keep-Alive Script\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepClicking(){\n",
    "    console.log(\"Keeping session alive...\");\n",
    "    document.querySelector(\"colab-connect-button\").click();\n",
    "}\n",
    "setInterval(KeepClicking, 60000);\n",
    "'''))\n",
    "\n",
    "print(\"‚úÖ Keep-alive activated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 8: FIX DRIVE TIMEOUT ISSUE (RUN FIRST!)\n",
    "\n",
    "**üî¥ CRITICAL: Run this BEFORE resuming extraction!**\n",
    "\n",
    "Your `datasets/EXTRACTED_FILINGS/10-K/` folder has ~75,000 files in it.\n",
    "Google Drive cannot handle >10,000 files in one folder.\n",
    "\n",
    "This section will:\n",
    "1. Reorganize existing files into year subfolders (2000/, 2001/, etc.)\n",
    "2. Patch extraction to write new files to year subfolders\n",
    "3. Update progress checker to count across all subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üî¥ STEP 1: Reorganize Existing Files into Year Subfolders\n",
    "## RUN: ONE TIME - Fixes Drive timeout by moving files\n",
    "##\n",
    "## What it does:\n",
    "## - Moves all JSON files from datasets/EXTRACTED_FILINGS/10-K/ into year subfolders\n",
    "## - Creates: 10-K/2000/, 10-K/2001/, ..., 10-K/2024/\n",
    "## - Extracts year from filename (e.g., 1234567_10K_2015_xxx.json ‚Üí 2015/)\n",
    "##\n",
    "## Expected time: 10-30 minutes for ~75,000 files\n",
    "## SAFE: Only moves files, does not delete anything\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "print(\"üîß REORGANIZING FILES TO FIX DRIVE TIMEOUT ISSUE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will move ~75,000 files into year subfolders\")\n",
    "print(\"Expected time: 10-30 minutes\\n\")\n",
    "\n",
    "base_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    print(f\"‚ùå Directory not found: {base_dir}\")\n",
    "else:\n",
    "    print(\"üìä Step 1/4: Scanning for files...\")\n",
    "    \n",
    "    # Get all JSON files in root directory only\n",
    "    try:\n",
    "        all_items = os.listdir(base_dir)\n",
    "        root_files = [f for f in all_items if os.path.isfile(os.path.join(base_dir, f)) and f.endswith('.json')]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error listing directory: {e}\")\n",
    "        print(\"   Drive may be temporarily unavailable - wait and retry\")\n",
    "        root_files = []\n",
    "    \n",
    "    print(f\"   Found {len(root_files):,} files to reorganize\")\n",
    "    \n",
    "    if len(root_files) == 0:\n",
    "        print(\"\\n‚úÖ No files to reorganize - already organized!\")\n",
    "    else:\n",
    "        print(f\"\\nüìä Step 2/4: Grouping by year...\")\n",
    "        \n",
    "        # Extract years\n",
    "        year_pattern = re.compile(r'_10K_(\\d{4})_')\n",
    "        year_groups = {}\n",
    "        no_year = []\n",
    "        \n",
    "        for filename in root_files:\n",
    "            match = year_pattern.search(filename)\n",
    "            if match:\n",
    "                year = match.group(1)\n",
    "                if year not in year_groups:\n",
    "                    year_groups[year] = []\n",
    "                year_groups[year].append(filename)\n",
    "            else:\n",
    "                no_year.append(filename)\n",
    "        \n",
    "        print(f\"   Years found: {len(year_groups)}\")\n",
    "        for year in sorted(year_groups.keys()):\n",
    "            print(f\"      {year}: {len(year_groups[year]):,} files\")\n",
    "        \n",
    "        if no_year:\n",
    "            print(f\"      Unknown: {len(no_year)} files (will skip)\")\n",
    "        \n",
    "        print(f\"\\nüöÄ Step 3/4: Moving files to year subfolders...\")\n",
    "        print(f\"   This will take 10-30 minutes\\n\")\n",
    "        \n",
    "        moved_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for year in sorted(year_groups.keys()):\n",
    "            year_dir = os.path.join(base_dir, year)\n",
    "            \n",
    "            # Create year folder\n",
    "            try:\n",
    "                os.makedirs(year_dir, exist_ok=True)\n",
    "                time.sleep(0.1)  # Small delay to avoid API rate limits\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error creating {year}/: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Move files with progress bar\n",
    "            files = year_groups[year]\n",
    "            pbar = tqdm(files, desc=f\"Year {year}\", leave=False)\n",
    "            \n",
    "            for filename in pbar:\n",
    "                src = os.path.join(base_dir, filename)\n",
    "                dst = os.path.join(year_dir, filename)\n",
    "                \n",
    "                try:\n",
    "                    os.rename(src, dst)\n",
    "                    moved_count += 1\n",
    "                    \n",
    "                    # Add small delay every 100 files to avoid overwhelming Drive\n",
    "                    if moved_count % 100 == 0:\n",
    "                        time.sleep(0.5)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    if error_count <= 5:\n",
    "                        print(f\"\\n‚ö†Ô∏è Error moving {filename}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüìä Step 4/4: Verifying reorganization...\")\n",
    "        \n",
    "        total_in_subfolders = 0\n",
    "        for year in sorted(year_groups.keys()):\n",
    "            year_dir = os.path.join(base_dir, year)\n",
    "            if os.path.exists(year_dir):\n",
    "                try:\n",
    "                    count = len([f for f in os.listdir(year_dir) if f.endswith('.json')])\n",
    "                    total_in_subfolders += count\n",
    "                    print(f\"   {year}/: {count:,} files\")\n",
    "                except:\n",
    "                    print(f\"   {year}/: (error counting)\")\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 60)\n",
    "        print(f\"‚úÖ REORGANIZATION COMPLETE!\")\n",
    "        print(f\"   Moved: {moved_count:,} files\")\n",
    "        print(f\"   Verified: {total_in_subfolders:,} files in subfolders\")\n",
    "        if error_count > 0:\n",
    "            print(f\"   ‚ö†Ô∏è Errors: {error_count} files failed\")\n",
    "        if no_year:\n",
    "            print(f\"   ‚ö†Ô∏è Skipped: {len(no_year)} files (no year)\")\n",
    "        print(f\"\\nüéâ You can now resume extraction!\")\n",
    "        print(f\"   New extractions will automatically use year subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üî¥ STEP 2: Patch Extraction to Write to Year Subfolders\n",
    "## RUN: ONE TIME - After reorganization\n",
    "##\n",
    "## What it does:\n",
    "## - Modifies extract_items.py to organize future extractions by year\n",
    "## - New files will automatically go to year subfolders\n",
    "## - Prevents the same timeout issue from happening again\n",
    "\n",
    "import os\n",
    "\n",
    "file_path = 'extract_items.py'\n",
    "\n",
    "print(\"üîß Patching extract_items.py for year subfolder organization...\\n\")\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Check if already patched\n",
    "if 'year_subfolder' in content or '# Year-based organization' in content:\n",
    "    print(\"‚úÖ extract_items.py already patched for year subfolders\")\n",
    "    print(\"   Future extractions will use year organization\")\n",
    "else:\n",
    "    print(\"Searching for patch location...\")\n",
    "    \n",
    "    # Find where output filename is created\n",
    "    old_code = '''        absolute_json_filename = os.path.join(\n",
    "            filing_type_folder, json_filename\n",
    "        )'''\n",
    "    \n",
    "    new_code = '''        # Year-based organization to avoid Drive folder limits\n",
    "        import re\n",
    "        year_match = re.search(r'_10K_(\\\\d{4})_', json_filename)\n",
    "        if year_match:\n",
    "            year_subfolder = year_match.group(1)\n",
    "            year_folder = os.path.join(filing_type_folder, year_subfolder)\n",
    "            os.makedirs(year_folder, exist_ok=True)\n",
    "            absolute_json_filename = os.path.join(year_folder, json_filename)\n",
    "        else:\n",
    "            # Fallback: no year found, use root\n",
    "            absolute_json_filename = os.path.join(filing_type_folder, json_filename)'''\n",
    "    \n",
    "    if old_code in content:\n",
    "        content = content.replace(old_code, new_code)\n",
    "        \n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        \n",
    "        print(\"‚úÖ extract_items.py successfully patched!\")\n",
    "        print(\"   Future extractions will write to year subfolders:\")\n",
    "        print(\"   - datasets/EXTRACTED_FILINGS/10-K/2024/\")\n",
    "        print(\"   - datasets/EXTRACTED_FILINGS/10-K/2023/\")\n",
    "        print(\"   - etc.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not find target code section\")\n",
    "        print(\"   Code structure may have changed\")\n",
    "        print(\"   Extraction might still work, but won't organize by year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: CHECK PROGRESS (UPDATED)\n",
    "\n",
    "**This version counts files across ALL year subfolders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîµ Check Extraction Progress (UPDATED FOR YEAR SUBFOLDERS)\n",
    "## RUN: When extraction is stopped\n",
    "##\n",
    "## Now counts files in:\n",
    "## - Root directory (old flat structure)\n",
    "## - Year subfolders (new organized structure)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
    "\n",
    "if os.path.exists(extracted_dir):\n",
    "    print(\"üìä Scanning for extracted files (including year subfolders)...\\n\")\n",
    "    \n",
    "    # Count all JSON files recursively\n",
    "    all_files = []\n",
    "    year_counts = {}\n",
    "    root_count = 0\n",
    "    \n",
    "    try:\n",
    "        for root, dirs, files in os.walk(extracted_dir):\n",
    "            json_files = [f for f in files if f.endswith('.json')]\n",
    "            all_files.extend([os.path.join(root, f) for f in json_files])\n",
    "            \n",
    "            # Track by location\n",
    "            if root == extracted_dir:\n",
    "                root_count = len(json_files)\n",
    "            else:\n",
    "                year = os.path.basename(root)\n",
    "                year_counts[year] = len(json_files)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scanning: {e}\")\n",
    "        print(\"   Drive may be temporarily slow - retry in a moment\")\n",
    "    \n",
    "    # Get expected total\n",
    "    metadata = pd.read_csv('datasets/FILINGS_METADATA.csv')\n",
    "    expected = len(metadata[metadata['Type'] == '10-K'])\n",
    "    \n",
    "    print(f\"üìä Extraction Progress:\")\n",
    "    print(f\"   Total Extracted: {len(all_files):,} files\")\n",
    "    print(f\"   Expected: {expected:,} files\")\n",
    "    print(f\"   Progress: {len(all_files)/expected*100:.1f}%\")\n",
    "    print(f\"   Remaining: {expected - len(all_files):,} files\")\n",
    "    \n",
    "    # Show organization\n",
    "    if year_counts or root_count > 0:\n",
    "        print(f\"\\nüìÅ File organization:\")\n",
    "        \n",
    "        if root_count > 0:\n",
    "            print(f\"   Root (not organized): {root_count:,} files\")\n",
    "            if root_count > 1000:\n",
    "                print(f\"      ‚ö†Ô∏è WARNING: Too many files in root!\")\n",
    "                print(f\"      Run reorganization script to fix this\")\n",
    "        \n",
    "        if year_counts:\n",
    "            print(f\"   Year subfolders: {sum(year_counts.values()):,} files\")\n",
    "            for year in sorted(year_counts.keys()):\n",
    "                print(f\"      {year}/: {year_counts[year]:,} files\")\n",
    "    \n",
    "    # Sample quality check\n",
    "    if len(all_files) > 0:\n",
    "        print(f\"\\nüìã Sample Quality Check (3 random files):\")\n",
    "        import random\n",
    "        sample = random.sample(all_files, min(3, len(all_files)))\n",
    "        \n",
    "        for fpath in sample:\n",
    "            fname = os.path.basename(fpath)\n",
    "            try:\n",
    "                with open(fpath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    has_mda = 'item_7' in data and len(data.get('item_7', '')) > 100\n",
    "                    mda_len = len(data.get('item_7', ''))\n",
    "                    print(f\"   {fname}: {'‚úÖ' if has_mda else '‚ùå'} MD&A ({mda_len:,} chars)\")\n",
    "            except Exception as e:\n",
    "                print(f\"   {fname}: ‚ö†Ô∏è Error - {e}\")\n",
    "                \n",
    "else:\n",
    "    print(\"‚ùå No extraction directory found\")\n",
    "    print(f\"   Expected: {extracted_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4: RESUME EXTRACTION\n",
    "\n",
    "**After running reorganization above, resume here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Resume MD&A Extraction\n",
    "## RUN: After reorganization is complete\n",
    "##\n",
    "## Now writes to year subfolders automatically\n",
    "## Should not experience Drive timeouts anymore\n",
    "\n",
    "print(\"üöÄ Resuming MD&A extraction...\")\n",
    "print(\"   Files will be organized by year\")\n",
    "print(\"   Should avoid Drive timeout issues\\n\")\n",
    "\n",
    "!python flexible_extractor.py --config extraction_configs/mda_only.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
