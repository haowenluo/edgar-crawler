{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkG53qSO2J7-"
      },
      "source": [
        "# CLEAN COLAB NOTEBOOK FOR MD&A EXTRACTION FROM EDGAR 10-K FILINGS\n"
      ],
      "id": "gkG53qSO2J7-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpp8l9ch2J7_"
      },
      "source": [
        "# SECTION 1: SETUP\n"
      ],
      "id": "gpp8l9ch2J7_"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PhBit-F22J7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a095f3-17ab-425a-a1db-50dd179d458b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Drive remounted successfully\n"
          ]
        }
      ],
      "source": [
        "## Cell 1: Mount Google Drive\n",
        "\n",
        "import os\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"âœ… Drive remounted successfully\")\n",
        "else:\n",
        "    print(\"âŒ Drive still not accessible\")"
      ],
      "id": "PhBit-F22J7_"
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 1.1: Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj6z4CwMBXD8",
        "outputId": "0e7e22df-0085-412c-8e18-fd8138a2b762"
      },
      "id": "Xj6z4CwMBXD8",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KD18o-ue2J7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ac12cb-2e80-4122-fc5b-6fa95cfd0edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/EDGAR_Project/edgar-crawler\n"
          ]
        }
      ],
      "source": [
        "## Cell 2: Navigate to Project Directory\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/EDGAR_Project/edgar-crawler')\n",
        "!pwd  # Verify location\n"
      ],
      "id": "KD18o-ue2J7_"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YS7uZR8V2J8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d47ec5f-dd4f-4686-fd10-1811a7f13544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing dependencies...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All dependencies installed\n"
          ]
        }
      ],
      "source": [
        "## Cell 3: Install Dependencies\n",
        "print(\"ğŸ“¦ Installing dependencies...\")\n",
        "\n",
        "# Install compatible versions to avoid conflicts\n",
        "!pip install -q 'dill<0.3.9' 'multiprocess<0.70.17'\n",
        "!pip install -q pox ppft\n",
        "!pip install -q --no-deps pathos\n",
        "!pip install -q beautifulsoup4 lxml requests pandas tqdm click cssutils numpy\n",
        "\n",
        "print(\"âœ… All dependencies installed\")\n"
      ],
      "id": "YS7uZR8V2J8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5rPgO1T2J8A"
      },
      "source": [
        "# SECTION 2: REBUILD METADATA (ONE-TIME SETUP)\n"
      ],
      "id": "I5rPgO1T2J8A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI34IoOL2J8A"
      },
      "outputs": [],
      "source": [
        "## Cell 4: Download Metadata Rebuilder Script\n",
        "!wget -q https://raw.githubusercontent.com/haowenluo/edgar-crawler/claude/fix-edgar-inventory-01VHYDnmaNQtFpWqjL8B6Bwq/rebuild_metadata_colab.py\n",
        "print(\"âœ… Script downloaded\")\n"
      ],
      "id": "tI34IoOL2J8A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "augRZpSp2J8A"
      },
      "outputs": [],
      "source": [
        "## Cell 5: Rebuild Metadata from All Files on Disk\n",
        "from rebuild_metadata_colab import rebuild_for_colab\n",
        "import pandas as pd\n",
        "\n",
        "# Fast mode: extracts CIK, Type, year, accession_number from filenames (~5-10 min)\n",
        "rebuild_for_colab(filing_types=['10-K'], fast_mode=True, dry_run=False)\n",
        "\n",
        "# Add required columns for extraction\n",
        "metadata = pd.read_csv('datasets/FILINGS_METADATA.csv')\n",
        "\n",
        "# Add all missing columns\n",
        "required_columns = {\n",
        "    'Company': lambda: 'Company_' + metadata['CIK'].astype(str),\n",
        "    'Date': lambda: metadata['year'].astype(str) + '-01-01',\n",
        "    'filing_date': lambda: metadata['year'].astype(str) + '-01-01',\n",
        "    'Period of Report': lambda: metadata['year'].astype(str) + '-12-31',\n",
        "    'SIC': lambda: 'Unknown',\n",
        "    'State of Inc': lambda: 'Unknown',\n",
        "    'State location': lambda: 'Unknown',\n",
        "    'Fiscal Year End': lambda: '1231',\n",
        "    'html_index': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
        "    'complete_text_file_link': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
        "    'htm_file_link': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
        "}\n",
        "\n",
        "for col_name, col_value_func in required_columns.items():\n",
        "    if col_name not in metadata.columns:\n",
        "        metadata[col_name] = col_value_func()\n",
        "\n",
        "metadata.to_csv('datasets/FILINGS_METADATA.csv', index=False)\n",
        "\n",
        "print(f\"\\nâœ… Metadata complete: {len(metadata):,} filings ready for extraction\")\n",
        "print(f\"   Unique companies: {metadata['CIK'].nunique():,}\")\n",
        "print(f\"   Years: {sorted(metadata['year'].unique())}\")\n"
      ],
      "id": "augRZpSp2J8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo6owr_g2J8B"
      },
      "source": [
        "# SECTION 3: CONFIGURE EXTRACTION\n"
      ],
      "id": "Fo6owr_g2J8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpPAwYUp2J8B"
      },
      "outputs": [],
      "source": [
        "## Cell 6: Update Config to Use Rebuilt Metadata\n",
        "import json\n",
        "\n",
        "config_path = 'config.json'\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "config['extract_items']['filings_metadata_file'] = 'FILINGS_METADATA.csv'\n",
        "config['extract_items']['filing_types'] = ['10-K']\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"âœ… Config updated for MD&A extraction\")\n"
      ],
      "id": "CpPAwYUp2J8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdMdYjrH2J8B"
      },
      "outputs": [],
      "source": [
        "## Cell 7: Apply Fix for Year-Based Subdirectories\n",
        "file_path = 'extract_items.py'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Only apply if not already patched\n",
        "if 'search in year subdirectories' not in content:\n",
        "    old_code = '''        absolute_filename = os.path.join(\n",
        "            self.raw_files_folder, filing_metadata[\"Type\"], filing_metadata[\"filename\"]\n",
        "        )\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(absolute_filename, \"r\", errors=\"backslashreplace\") as file:\n",
        "            content = file.read()'''\n",
        "\n",
        "    new_code = '''        # First try direct path (original behavior)\n",
        "        absolute_filename = os.path.join(\n",
        "            self.raw_files_folder, filing_metadata[\"Type\"], filing_metadata[\"filename\"]\n",
        "        )\n",
        "\n",
        "        # If file doesn't exist, search in year subdirectories\n",
        "        if not os.path.exists(absolute_filename):\n",
        "            type_dir = os.path.join(self.raw_files_folder, filing_metadata[\"Type\"])\n",
        "            found = False\n",
        "            for root, dirs, files in os.walk(type_dir):\n",
        "                if filing_metadata[\"filename\"] in files:\n",
        "                    absolute_filename = os.path.join(root, filing_metadata[\"filename\"])\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            if not found:\n",
        "                raise FileNotFoundError(f\"Could not find {filing_metadata['filename']} in {type_dir}\")\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(absolute_filename, \"r\", errors=\"backslashreplace\") as file:\n",
        "            content = file.read()'''\n",
        "\n",
        "    content = content.replace(old_code, new_code)\n",
        "\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    print(\"âœ… extract_items.py patched for nested directories\")\n",
        "else:\n",
        "    print(\"âœ… extract_items.py already patched\")\n"
      ],
      "id": "DdMdYjrH2J8B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyyvWeMs2J8B"
      },
      "source": [
        "# SECTION 4: EXTRACT MD&A\n"
      ],
      "id": "dyyvWeMs2J8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bdTELG_2J8B"
      },
      "outputs": [],
      "source": [
        "## Cell 8: Run MD&A Extraction\n",
        "!python flexible_extractor.py --config extraction_configs/mda_only.json\n"
      ],
      "id": "5bdTELG_2J8B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9Nc1Fvi2J8C"
      },
      "source": [
        "# SECTION 5: CHECK PROGRESS\n"
      ],
      "id": "d9Nc1Fvi2J8C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUF-HUU72J8C"
      },
      "outputs": [],
      "source": [
        "## Cell 9: Check Extraction Progress\n",
        "import os\n",
        "import json\n",
        "\n",
        "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
        "\n",
        "if os.path.exists(extracted_dir):\n",
        "    all_files = [f for f in os.listdir(extracted_dir) if f.endswith('.json')]\n",
        "\n",
        "    # Get expected total from metadata\n",
        "    metadata = pd.read_csv('datasets/FILINGS_METADATA.csv')\n",
        "    expected = len(metadata[metadata['Type'] == '10-K'])\n",
        "\n",
        "    print(f\"ğŸ“Š Extraction Progress:\")\n",
        "    print(f\"   Extracted: {len(all_files):,} files\")\n",
        "    print(f\"   Expected: {expected:,} files\")\n",
        "    print(f\"   Progress: {len(all_files)/expected*100:.1f}%\")\n",
        "    print(f\"   Remaining: {expected - len(all_files):,} files\")\n",
        "\n",
        "    # Check sample files for MD&A content\n",
        "    print(f\"\\nğŸ“‹ Sample Quality Check:\")\n",
        "    for fname in all_files[:3]:\n",
        "        fpath = os.path.join(extracted_dir, fname)\n",
        "        with open(fpath, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            has_mda = 'item_7' in data and len(data.get('item_7', '')) > 100\n",
        "            mda_len = len(data.get('item_7', ''))\n",
        "            print(f\"   {fname}: {'âœ…' if has_mda else 'âŒ'} MD&A ({mda_len:,} chars)\")\n",
        "else:\n",
        "    print(\"âŒ No extraction directory found\")\n"
      ],
      "id": "lUF-HUU72J8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyKN76mY2J8C"
      },
      "source": [
        "# SECTION 6: CREATE ANALYSIS FILES\n"
      ],
      "id": "HyKN76mY2J8C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPu2FuTo2J8C"
      },
      "outputs": [],
      "source": [
        "## Cell 10: Create Metadata CSV and Parquet for Analysis\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"ğŸ“Š Creating analysis files...\")\n",
        "\n",
        "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
        "metadata_records = []\n",
        "full_data_records = []\n",
        "\n",
        "# Process all extracted JSON files\n",
        "json_files = [f for f in os.listdir(extracted_dir) if f.endswith('.json')]\n",
        "\n",
        "for filename in tqdm(json_files, desc=\"Processing\"):\n",
        "    filepath = os.path.join(extracted_dir, filename)\n",
        "\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            filing = json.load(f)\n",
        "\n",
        "        # Metadata (lightweight)\n",
        "        metadata_records.append({\n",
        "            'filename': filename,\n",
        "            'cik': filing.get('cik', ''),\n",
        "            'company': filing.get('company', ''),\n",
        "            'filing_date': filing.get('filing_date', ''),\n",
        "            'period_of_report': filing.get('period_of_report', ''),\n",
        "            'year': filing.get('period_of_report', '')[:4] if filing.get('period_of_report', '') else '',\n",
        "            'has_mda': 'item_7' in filing and len(filing.get('item_7', '')) > 0,\n",
        "            'mda_length': len(filing.get('item_7', '')),\n",
        "            'json_path': filepath\n",
        "        })\n",
        "\n",
        "        # Full data (with MD&A text)\n",
        "        if 'item_7' in filing and filing['item_7']:\n",
        "            full_data_records.append({\n",
        "                'cik': filing.get('cik', ''),\n",
        "                'company': filing.get('company', ''),\n",
        "                'filing_date': filing.get('filing_date', ''),\n",
        "                'period_of_report': filing.get('period_of_report', ''),\n",
        "                'year': filing.get('period_of_report', '')[:4] if filing.get('period_of_report', '') else '',\n",
        "                'mda_text': filing.get('item_7', '')\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Error: {filename} - {e}\")\n",
        "\n",
        "# Save files\n",
        "df_meta = pd.DataFrame(metadata_records)\n",
        "df_full = pd.DataFrame(full_data_records)\n",
        "\n",
        "meta_path = '/content/drive/MyDrive/EDGAR_Project/mda_metadata.csv'\n",
        "parquet_path = '/content/drive/MyDrive/EDGAR_Project/mda_full.parquet'\n",
        "\n",
        "df_meta.to_csv(meta_path, index=False)\n",
        "df_full.to_parquet(parquet_path, compression='gzip', index=False)\n",
        "\n",
        "print(f\"\\nâœ… Files created:\")\n",
        "print(f\"   ğŸ“„ Metadata CSV: {len(df_meta):,} records ({os.path.getsize(meta_path)/1024:.1f} KB)\")\n",
        "print(f\"   ğŸ“¦ Parquet: {len(df_full):,} records ({os.path.getsize(parquet_path)/(1024**2):.1f} MB)\")\n",
        "print(f\"\\nğŸ“Š Statistics:\")\n",
        "print(f\"   Years covered: {sorted(df_full['year'].unique())}\")\n",
        "print(f\"   MD&A length: {df_full['mda_text'].str.len().describe()}\")\n",
        "print(f\"\\nğŸ‰ Ready for analysis!\")\n"
      ],
      "id": "KPu2FuTo2J8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNszAcrF2J8C"
      },
      "source": [
        "# OPTIONAL: RESUME EXTRACTION AFTER INTERRUPTION\n"
      ],
      "id": "pNszAcrF2J8C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2awzQJwC2J8C"
      },
      "outputs": [],
      "source": [
        "## Cell 11: Resume Extraction (Run this if Colab disconnects)\n",
        "# Reinstall dependencies\n",
        "!pip install -q 'dill<0.3.9' 'multiprocess<0.70.17' pox ppft\n",
        "!pip install -q --no-deps pathos\n",
        "!pip install -q beautifulsoup4 lxml requests pandas tqdm click cssutils numpy\n",
        "\n",
        "# Resume extraction\n",
        "!python flexible_extractor.py --config extraction_configs/mda_only.json\n"
      ],
      "id": "2awzQJwC2J8C"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}