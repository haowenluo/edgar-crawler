{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEAN COLAB NOTEBOOK FOR MD&A EXTRACTION FROM EDGAR 10-K FILINGS\n",
    "\n",
    "**Purpose:** Extract Management's Discussion and Analysis (MD&A) sections from 10-K filings\n",
    "\n",
    "**Prerequisites:**\n",
    "- Repository already cloned to: `/content/drive/MyDrive/EDGAR_Project/edgar-crawler`\n",
    "- Raw 10-K files downloaded and stored in Google Drive\n",
    "\n",
    "**Instructions:**\n",
    "- üü¢ **GREEN cells** = Run EVERY TIME (including resume)\n",
    "- üü° **YELLOW cells** = Run FIRST TIME ONLY (has skip logic)\n",
    "- üîµ **BLUE cells** = Run ONLY WHEN NEEDED (optional/conditional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: SETUP\n",
    "\n",
    "These cells prepare your Colab environment and connect to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 1: Mount Google Drive\n",
    "## RUN: EVERY TIME (first step for any session)\n",
    "##\n",
    "## What it does:\n",
    "## - Connects Colab to your Google Drive\n",
    "## - Allows access to repository and data files\n",
    "## - Checks if already mounted to avoid duplicate mount attempts\n",
    "##\n",
    "## Expected output: \"‚úÖ Drive mounted successfully\" or \"‚úÖ Drive already mounted\"\n",
    "\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Check if already mounted\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"‚úÖ Drive already mounted\")\n",
    "else:\n",
    "    # Mount for first time\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 2: Navigate to Repository\n",
    "## RUN: EVERY TIME\n",
    "##\n",
    "## What it does:\n",
    "## - Changes working directory to your cloned repository\n",
    "## - All subsequent commands run from this directory\n",
    "## - Verifies you're in the correct location\n",
    "##\n",
    "## Expected output: /content/drive/MyDrive/EDGAR_Project/edgar-crawler\n",
    "##\n",
    "## NOTE: Repository should already be cloned here. If not, you need to clone it first!\n",
    "\n",
    "import os\n",
    "\n",
    "REPO_DIR = '/content/drive/MyDrive/EDGAR_Project/edgar-crawler'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    os.chdir(REPO_DIR)\n",
    "    print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"‚ùå Repository not found at: {REPO_DIR}\")\n",
    "    print(\"Please clone the repository first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 3: Install Dependencies\n",
    "## RUN: EVERY TIME (if runtime was restarted)\n",
    "## SKIP: If runtime is still active and you just disconnected from Drive\n",
    "##\n",
    "## What it does:\n",
    "## - Installs Python packages needed for extraction\n",
    "## - Uses specific versions to avoid dependency conflicts\n",
    "## - Includes pyarrow for Parquet file creation\n",
    "##\n",
    "## How to know if you need to run:\n",
    "## - If you see \"Runtime disconnected\" or \"Session crashed\" ‚Üí RUN THIS\n",
    "## - If you only see \"Drive disconnected\" ‚Üí SKIP THIS (dependencies still there)\n",
    "##\n",
    "## Expected time: ~30-60 seconds\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Install compatible versions to avoid conflicts\n",
    "!pip install -q 'dill<0.3.9' 'multiprocess<0.70.17'\n",
    "!pip install -q pox ppft\n",
    "!pip install -q --no-deps pathos  # No-deps avoids conflicts\n",
    "!pip install -q beautifulsoup4 lxml requests pandas tqdm click cssutils numpy\n",
    "!pip install -q pyarrow  # For Parquet file creation\n",
    "\n",
    "print(\"‚úÖ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 4: Activate Keep-Alive Script\n",
    "## RUN: EVERY TIME (recommended)\n",
    "##\n",
    "## What it does:\n",
    "## - Simulates browser activity to prevent Colab from disconnecting\n",
    "## - Clicks the connect button every 60 seconds\n",
    "## - Does NOT guarantee no disconnection (but significantly helps!)\n",
    "##\n",
    "## Benefits:\n",
    "## - Reduces disconnections during long extractions\n",
    "## - Keeps session alive even if you switch browser tabs\n",
    "##\n",
    "## Expected output: \"‚úÖ Keep-alive activated\"\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepClicking(){\n",
    "    console.log(\"Keeping session alive...\");\n",
    "    document.querySelector(\"colab-connect-button\").click();\n",
    "}\n",
    "setInterval(KeepClicking, 60000);  // Click every 60 seconds\n",
    "'''))\n",
    "\n",
    "print(\"‚úÖ Keep-alive activated\")\n",
    "print(\"üí° This helps prevent disconnection during long extractions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: REBUILD METADATA (ONE-TIME SETUP)\n",
    "\n",
    "**Run this section ONLY on first extraction.**\n",
    "\n",
    "These cells create a metadata CSV file by scanning all downloaded 10-K files on disk.\n",
    "Once created, you can skip this section for all future extractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü° Cell 5: Rebuild Metadata from Files on Disk\n",
    "## RUN: FIRST TIME ONLY\n",
    "## SKIP: If datasets/FILINGS_METADATA.csv already exists\n",
    "##\n",
    "## What it does:\n",
    "## - Scans all downloaded 10-K files in RAW_FILINGS/10-K/\n",
    "## - Extracts CIK, year, accession number from filenames\n",
    "## - Creates FILINGS_METADATA.csv with all required columns\n",
    "## - This CSV tells the extractor which files to process\n",
    "##\n",
    "## Expected time: 5-10 minutes for ~80,000 files\n",
    "##\n",
    "## When to re-run:\n",
    "## - First time ever\n",
    "## - If you downloaded NEW 10-K files\n",
    "## - If FILINGS_METADATA.csv is missing or corrupted\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "METADATA_FILE = 'datasets/FILINGS_METADATA.csv'\n",
    "\n",
    "# Check if metadata already exists\n",
    "if os.path.exists(METADATA_FILE):\n",
    "    metadata = pd.read_csv(METADATA_FILE)\n",
    "    print(f\"‚úÖ Metadata already exists: {len(metadata):,} filings\")\n",
    "    print(f\"   To rebuild, delete {METADATA_FILE} and re-run this cell\")\n",
    "else:\n",
    "    print(\"üìä Building metadata from files on disk...\")\n",
    "    print(\"   This may take 5-10 minutes for large datasets\\n\")\n",
    "    \n",
    "    from rebuild_metadata_colab import rebuild_for_colab\n",
    "    \n",
    "    # Fast mode: extracts CIK, Type, year, accession_number from filenames\n",
    "    rebuild_for_colab(filing_types=['10-K'], fast_mode=True, dry_run=False)\n",
    "    \n",
    "    # Add required columns for extraction\n",
    "    metadata = pd.read_csv(METADATA_FILE)\n",
    "    \n",
    "    # Add all missing columns with placeholder data\n",
    "    required_columns = {\n",
    "        'Company': lambda: 'Company_' + metadata['CIK'].astype(str),\n",
    "        'Date': lambda: metadata['year'].astype(str) + '-01-01',\n",
    "        'filing_date': lambda: metadata['year'].astype(str) + '-01-01',\n",
    "        'Period of Report': lambda: metadata['year'].astype(str) + '-12-31',\n",
    "        'SIC': lambda: 'Unknown',\n",
    "        'State of Inc': lambda: 'Unknown',\n",
    "        'State location': lambda: 'Unknown',\n",
    "        'Fiscal Year End': lambda: '1231',\n",
    "        'html_index': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
    "        'complete_text_file_link': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
    "        'htm_file_link': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
    "    }\n",
    "    \n",
    "    for col_name, col_value_func in required_columns.items():\n",
    "        if col_name not in metadata.columns:\n",
    "            metadata[col_name] = col_value_func()\n",
    "    \n",
    "    metadata.to_csv(METADATA_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Metadata complete: {len(metadata):,} filings ready for extraction\")\n",
    "    print(f\"   Unique companies: {metadata['CIK'].nunique():,}\")\n",
    "    print(f\"   Years: {sorted(metadata['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: CONFIGURE EXTRACTION\n",
    "\n",
    "These cells configure the extraction settings. Run once, then can skip on resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü° Cell 6: Update Config for MD&A Extraction\n",
    "## RUN: FIRST TIME ONLY\n",
    "## SKIP: On resume (config already set)\n",
    "##\n",
    "## What it does:\n",
    "## - Updates config.json to point to your metadata file\n",
    "## - Sets filing type to 10-K only\n",
    "## - Ensures extractor knows where to find filing information\n",
    "##\n",
    "## Expected output: \"‚úÖ Config updated for MD&A extraction\"\n",
    "##\n",
    "## When to re-run:\n",
    "## - First time\n",
    "## - If you pulled new code from repository that changed config.json\n",
    "\n",
    "import json\n",
    "\n",
    "config_path = 'config.json'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config['extract_items']['filings_metadata_file'] = 'FILINGS_METADATA.csv'\n",
    "config['extract_items']['filing_types'] = ['10-K']\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Config updated for MD&A extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü° Cell 7: Verify Subdirectory Support\n",
    "## RUN: FIRST TIME ONLY (automatic check)\n",
    "## SKIP: On resume\n",
    "##\n",
    "## What it does:\n",
    "## - Checks if extract_items.py supports year-based subdirectories\n",
    "## - Main branch should already have this fix\n",
    "## - If not found, displays a warning (but likely won't be needed)\n",
    "##\n",
    "## Background:\n",
    "## - Some 10-K files are organized in year folders (e.g., 2007/, 2008/)\n",
    "## - This check ensures the extractor can find files in subdirectories\n",
    "##\n",
    "## Expected output: \"‚úÖ extract_items.py supports nested directories\"\n",
    "\n",
    "file_path = 'extract_items.py'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Check if already supports subdirectories\n",
    "if 'os.walk(type_dir)' in content:\n",
    "    print(\"‚úÖ extract_items.py supports nested directories\")\n",
    "    print(\"   Can find files in year-based subdirectories (e.g., 2007/, 2008/)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  extract_items.py may not support nested directories\")\n",
    "    print(\"   Main branch should have this fix - try pulling latest code\")\n",
    "    print(\"   If extraction fails with 'FileNotFoundError', update from main branch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 4: EXTRACT MD&A\n",
    "\n",
    "**Main extraction cell - Run every time you want to extract or resume.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üü¢ Cell 8: Run MD&A Extraction\n",
    "## RUN: EVERY TIME (for first run AND resume)\n",
    "##\n",
    "## What it does:\n",
    "## - Extracts Item 7 (MD&A section) from all 10-K filings\n",
    "## - Automatically SKIPS already-extracted files (resume-friendly!)\n",
    "## - Saves results to datasets/EXTRACTED_FILINGS/10-K/ as JSON files\n",
    "##\n",
    "## Expected behavior:\n",
    "## - First run: Starts from file 1, extracts all\n",
    "## - Resume: Quickly skips already-done files, continues from where it stopped\n",
    "## - Progress bar shows: X/83628 files processed\n",
    "## - Speed: ~10-18 files/second (varies by file size)\n",
    "##\n",
    "## How resume works:\n",
    "## - \"Skip Existing: True\" is set in extraction_configs/mda_only.json\n",
    "## - Script checks if output JSON exists before processing\n",
    "## - Skipped files process in ~0.01 seconds (very fast)\n",
    "## - New extractions take ~5 seconds each (slower)\n",
    "##\n",
    "## Expected time:\n",
    "## - First full run: ~12-24 hours for 83,628 files\n",
    "## - Resume after 50% done: ~6-12 hours\n",
    "##\n",
    "## If disconnected:\n",
    "## 1. Re-run Cells 1-4 (Mount, Navigate, Dependencies, Keep-alive)\n",
    "## 2. Re-run this cell - it will resume automatically!\n",
    "##\n",
    "## Expected output:\n",
    "## - Progress bar moving (even if starting at 0%)\n",
    "## - \"Could not extract\" messages (normal - some files lack MD&A)\n",
    "## - Speed indicator (e.g., 17.93it/s)\n",
    "\n",
    "print(\"üöÄ Starting MD&A extraction...\")\n",
    "print(\"   This will automatically skip already-extracted files\")\n",
    "print(\"   Safe to resume after disconnection!\\n\")\n",
    "\n",
    "!python flexible_extractor.py --config extraction_configs/mda_only.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: CHECK PROGRESS\n",
    "\n",
    "**Use these cells to monitor extraction progress.**\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT:** You can ONLY run these cells when Cell 8 (extraction) is NOT running!\n",
    "Colab can only run one cell at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîµ Cell 9: Check Extraction Progress\n",
    "## RUN: ONLY when extraction cell (Cell 8) is stopped/completed\n",
    "##\n",
    "## What it does:\n",
    "## - Counts how many JSON files have been extracted\n",
    "## - Compares against total expected files from metadata\n",
    "## - Shows percentage complete and files remaining\n",
    "## - Checks sample files for MD&A quality\n",
    "##\n",
    "## When to run:\n",
    "## - BEFORE starting extraction (to see baseline)\n",
    "## - AFTER extraction completes\n",
    "## - AFTER a disconnection (to see how much was done)\n",
    "##\n",
    "## ‚ö†Ô∏è DO NOT run while Cell 8 is running! (Colab limitation)\n",
    "##\n",
    "## Expected output:\n",
    "## - Extracted: X files\n",
    "## - Expected: 83,628 files (or your total)\n",
    "## - Progress: X%\n",
    "## - Sample quality check showing 3 random files\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
    "\n",
    "if os.path.exists(extracted_dir):\n",
    "    # Count all JSON files (including in subdirectories if any)\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(extracted_dir):\n",
    "        all_files.extend([os.path.join(root, f) for f in files if f.endswith('.json')])\n",
    "    \n",
    "    # Get expected total from metadata\n",
    "    metadata = pd.read_csv('datasets/FILINGS_METADATA.csv')\n",
    "    expected = len(metadata[metadata['Type'] == '10-K'])\n",
    "    \n",
    "    print(f\"üìä Extraction Progress:\")\n",
    "    print(f\"   Extracted: {len(all_files):,} files\")\n",
    "    print(f\"   Expected: {expected:,} files\")\n",
    "    print(f\"   Progress: {len(all_files)/expected*100:.1f}%\")\n",
    "    print(f\"   Remaining: {expected - len(all_files):,} files\")\n",
    "    \n",
    "    # Check sample files for MD&A content\n",
    "    if len(all_files) > 0:\n",
    "        print(f\"\\nüìã Sample Quality Check (first 3 files):\")\n",
    "        for fpath in all_files[:3]:\n",
    "            fname = os.path.basename(fpath)\n",
    "            try:\n",
    "                with open(fpath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    has_mda = 'item_7' in data and len(data.get('item_7', '')) > 100\n",
    "                    mda_len = len(data.get('item_7', ''))\n",
    "                    print(f\"   {fname}: {'‚úÖ' if has_mda else '‚ùå'} MD&A ({mda_len:,} chars)\")\n",
    "            except Exception as e:\n",
    "                print(f\"   {fname}: ‚ö†Ô∏è Error reading file - {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No extraction directory found\")\n",
    "    print(f\"   Expected location: {extracted_dir}\")\n",
    "    print(\"   Run Cell 8 to start extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 6: CREATE ANALYSIS FILES\n",
    "\n",
    "**Run this AFTER extraction is complete to create analysis-ready files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîµ Cell 10: Create Analysis Files (CSV + Parquet)\n",
    "## RUN: AFTER extraction is complete (or mostly complete)\n",
    "##\n",
    "## What it does:\n",
    "## - Reads all extracted JSON files\n",
    "## - Creates TWO output files:\n",
    "##   1. mda_metadata.csv (lightweight, just metadata + stats)\n",
    "##   2. mda_full.parquet (full MD&A text, compressed, ~500MB-1GB)\n",
    "##\n",
    "## Output files:\n",
    "## - mda_metadata.csv: Filename, CIK, company, dates, MD&A length, etc.\n",
    "## - mda_full.parquet: Full MD&A text + metadata (for text analysis)\n",
    "##\n",
    "## Where saved:\n",
    "## - /content/drive/MyDrive/EDGAR_Project/ (your project root)\n",
    "##\n",
    "## When to run:\n",
    "## - After all extraction is done\n",
    "## - Can run on partial extractions (will process whatever exists)\n",
    "##\n",
    "## Expected time: 5-15 minutes for 80,000+ files\n",
    "##\n",
    "## Uses:\n",
    "## - CSV: Quick viewing in Excel/Google Sheets\n",
    "## - Parquet: Fast loading in Python/R for text analysis\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"üìä Creating analysis files...\")\n",
    "print(\"   This may take 5-15 minutes depending on file count\\n\")\n",
    "\n",
    "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
    "metadata_records = []\n",
    "full_data_records = []\n",
    "\n",
    "# Process all extracted JSON files\n",
    "json_files = []\n",
    "for root, dirs, files in os.walk(extracted_dir):\n",
    "    json_files.extend([os.path.join(root, f) for f in files if f.endswith('.json')])\n",
    "\n",
    "for filepath in tqdm(json_files, desc=\"Processing\"):\n",
    "    filename = os.path.basename(filepath)\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            filing = json.load(f)\n",
    "        \n",
    "        # Metadata (lightweight)\n",
    "        metadata_records.append({\n",
    "            'filename': filename,\n",
    "            'cik': filing.get('cik', ''),\n",
    "            'company': filing.get('company', ''),\n",
    "            'filing_date': filing.get('filing_date', ''),\n",
    "            'period_of_report': filing.get('period_of_report', ''),\n",
    "            'year': filing.get('period_of_report', '')[:4] if filing.get('period_of_report', '') else '',\n",
    "            'has_mda': 'item_7' in filing and len(filing.get('item_7', '')) > 0,\n",
    "            'mda_length': len(filing.get('item_7', '')),\n",
    "            'json_path': filepath\n",
    "        })\n",
    "        \n",
    "        # Full data (with MD&A text)\n",
    "        if 'item_7' in filing and filing['item_7']:\n",
    "            full_data_records.append({\n",
    "                'cik': filing.get('cik', ''),\n",
    "                'company': filing.get('company', ''),\n",
    "                'filing_date': filing.get('filing_date', ''),\n",
    "                'period_of_report': filing.get('period_of_report', ''),\n",
    "                'year': filing.get('period_of_report', '')[:4] if filing.get('period_of_report', '') else '',\n",
    "                'mda_text': filing.get('item_7', '')\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error processing {filename}: {e}\")\n",
    "\n",
    "# Save files\n",
    "df_meta = pd.DataFrame(metadata_records)\n",
    "df_full = pd.DataFrame(full_data_records)\n",
    "\n",
    "meta_path = '/content/drive/MyDrive/EDGAR_Project/mda_metadata.csv'\n",
    "parquet_path = '/content/drive/MyDrive/EDGAR_Project/mda_full.parquet'\n",
    "\n",
    "df_meta.to_csv(meta_path, index=False)\n",
    "df_full.to_parquet(parquet_path, compression='gzip', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Files created:\")\n",
    "print(f\"   üìÑ Metadata CSV: {len(df_meta):,} records ({os.path.getsize(meta_path)/1024:.1f} KB)\")\n",
    "print(f\"   üì¶ Parquet: {len(df_full):,} records ({os.path.getsize(parquet_path)/(1024**2):.1f} MB)\")\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Files with MD&A: {df_meta['has_mda'].sum():,}\")\n",
    "print(f\"   Files without MD&A: {(~df_meta['has_mda']).sum():,}\")\n",
    "if len(df_full) > 0:\n",
    "    print(f\"   Years covered: {sorted(df_full['year'].unique())}\")\n",
    "    print(f\"   Avg MD&A length: {df_full['mda_text'].str.len().mean():,.0f} chars\")\n",
    "print(f\"\\nüéâ Ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: QUICK RESUME (AFTER DISCONNECTION)\n",
    "\n",
    "**If Colab disconnects, run ONLY these cells to resume:**\n",
    "\n",
    "1. Cell 1 (Mount Drive)\n",
    "2. Cell 2 (Navigate to repo)\n",
    "3. Cell 3 (Install dependencies) - ONLY if runtime restarted\n",
    "4. Cell 4 (Keep-alive)\n",
    "5. Cell 8 (Resume extraction)\n",
    "\n",
    "**Skip Cells 5-7** (metadata rebuild and config - already done!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîµ Cell 11: Quick Resume Helper\n",
    "## RUN: OPTIONAL - Only if you want a one-click resume\n",
    "##\n",
    "## What it does:\n",
    "## - Combines all resume steps into one cell\n",
    "## - Checks Drive connection\n",
    "## - Navigates to repo\n",
    "## - Resumes extraction\n",
    "##\n",
    "## When to use:\n",
    "## - After a disconnection\n",
    "## - If you already ran dependencies (Cell 3) and they're still installed\n",
    "##\n",
    "## ‚ö†Ô∏è If this fails with import errors, run Cell 3 first!\n",
    "\n",
    "import os\n",
    "\n",
    "# Check Drive\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\"‚ùå Drive not mounted! Run Cell 1 first.\")\n",
    "else:\n",
    "    # Navigate to repo\n",
    "    os.chdir('/content/drive/MyDrive/EDGAR_Project/edgar-crawler')\n",
    "    print(f\"‚úÖ Ready to resume from: {os.getcwd()}\")\n",
    "    print(\"\\nüîÑ Resuming extraction...\\n\")\n",
    "    \n",
    "    # Resume extraction\n",
    "    !python flexible_extractor.py --config extraction_configs/mda_only.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
