{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl6MY4d_QUeI"
      },
      "source": [
        "# CLEAN COLAB NOTEBOOK FOR MD&A EXTRACTION FROM EDGAR 10-K FILINGS\n",
        "\n",
        "**Purpose:** Extract Management's Discussion and Analysis (MD&A) sections from 10-K filings\n",
        "\n",
        "**Prerequisites:**\n",
        "- Repository already cloned to: `/content/drive/MyDrive/EDGAR_Project/edgar-crawler`\n",
        "- Raw 10-K files downloaded and stored in Google Drive\n",
        "\n",
        "**Instructions:**\n",
        "- üü¢ **GREEN cells** = Run EVERY TIME (including resume)\n",
        "- üü° **YELLOW cells** = Run FIRST TIME ONLY (has skip logic)\n",
        "- üîµ **BLUE cells** = Run ONLY WHEN NEEDED (optional/conditional)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZF5KyPxQUeK"
      },
      "source": [
        "# SECTION 1: SETUP\n",
        "\n",
        "These cells prepare your Colab environment and connect to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ikNgysWsQUeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53cd852-f623-4c58-9639-1885bfb51e80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Drive mounted successfully\n"
          ]
        }
      ],
      "source": [
        "## üü¢ Cell 1: Mount Google Drive\n",
        "## RUN: EVERY TIME (first step for any session)\n",
        "##\n",
        "## What it does:\n",
        "## - Connects Colab to your Google Drive\n",
        "## - Allows access to repository and data files\n",
        "## - Checks if already mounted to avoid duplicate mount attempts\n",
        "##\n",
        "## Expected output: \"‚úÖ Drive mounted successfully\" or \"‚úÖ Drive already mounted\"\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Check if already mounted\n",
        "if os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"‚úÖ Drive already mounted\")\n",
        "else:\n",
        "    # Mount for first time\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Drive mounted successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj4mfJcWQUeL",
        "outputId": "512893aa-d744-4f1d-cfdb-5e69697d6494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Working directory: /content/drive/MyDrive/EDGAR_Project/edgar-crawler\n"
          ]
        }
      ],
      "source": [
        "## üü¢ Cell 2: Navigate to Repository\n",
        "## RUN: EVERY TIME\n",
        "##\n",
        "## What it does:\n",
        "## - Changes working directory to your cloned repository\n",
        "## - All subsequent commands run from this directory\n",
        "## - Verifies you're in the correct location\n",
        "##\n",
        "## Expected output: /content/drive/MyDrive/EDGAR_Project/edgar-crawler\n",
        "##\n",
        "## NOTE: Repository should already be cloned here. If not, you need to clone it first!\n",
        "\n",
        "import os\n",
        "\n",
        "REPO_DIR = '/content/drive/MyDrive/EDGAR_Project/edgar-crawler'\n",
        "\n",
        "if os.path.exists(REPO_DIR):\n",
        "    os.chdir(REPO_DIR)\n",
        "    print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"‚ùå Repository not found at: {REPO_DIR}\")\n",
        "    print(\"Please clone the repository first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn-RrYB4QUeM",
        "outputId": "246d4844-1e30-4bf4-d156-acc7e5cb843d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m385.7/385.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ All dependencies installed\n"
          ]
        }
      ],
      "source": [
        "## üü¢ Cell 3: Install Dependencies\n",
        "## RUN: EVERY TIME (if runtime was restarted)\n",
        "## SKIP: If runtime is still active and you just disconnected from Drive\n",
        "##\n",
        "## What it does:\n",
        "## - Installs Python packages needed for extraction\n",
        "## - Uses specific versions to avoid dependency conflicts\n",
        "## - Includes pyarrow for Parquet file creation\n",
        "##\n",
        "## How to know if you need to run:\n",
        "## - If you see \"Runtime disconnected\" or \"Session crashed\" ‚Üí RUN THIS\n",
        "## - If you only see \"Drive disconnected\" ‚Üí SKIP THIS (dependencies still there)\n",
        "##\n",
        "## Expected time: ~30-60 seconds\n",
        "\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "\n",
        "# Install compatible versions to avoid conflicts\n",
        "!pip install -q 'dill<0.3.9' 'multiprocess<0.70.17'\n",
        "!pip install -q pox ppft\n",
        "!pip install -q --no-deps pathos  # No-deps avoids conflicts\n",
        "!pip install -q beautifulsoup4 lxml requests pandas tqdm click cssutils numpy\n",
        "!pip install -q pyarrow  # For Parquet file creation\n",
        "\n",
        "print(\"‚úÖ All dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "rZU_JlRKQUeM",
        "outputId": "72d96636-7db8-431e-9149-dec0502db271"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function KeepClicking(){\n",
              "    console.log(\"Keeping session alive...\");\n",
              "    document.querySelector(\"colab-connect-button\").click();\n",
              "}\n",
              "setInterval(KeepClicking, 60000);  // Click every 60 seconds\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Keep-alive activated\n",
            "üí° This helps prevent disconnection during long extractions\n"
          ]
        }
      ],
      "source": [
        "## üü¢ Cell 4: Activate Keep-Alive Script\n",
        "## RUN: EVERY TIME (recommended)\n",
        "##\n",
        "## What it does:\n",
        "## - Simulates browser activity to prevent Colab from disconnecting\n",
        "## - Clicks the connect button every 60 seconds\n",
        "## - Does NOT guarantee no disconnection (but significantly helps!)\n",
        "##\n",
        "## Benefits:\n",
        "## - Reduces disconnections during long extractions\n",
        "## - Keeps session alive even if you switch browser tabs\n",
        "##\n",
        "## Expected output: \"‚úÖ Keep-alive activated\"\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "display(Javascript('''\n",
        "function KeepClicking(){\n",
        "    console.log(\"Keeping session alive...\");\n",
        "    document.querySelector(\"colab-connect-button\").click();\n",
        "}\n",
        "setInterval(KeepClicking, 60000);  // Click every 60 seconds\n",
        "'''))\n",
        "\n",
        "print(\"‚úÖ Keep-alive activated\")\n",
        "print(\"üí° This helps prevent disconnection during long extractions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8w-WAzIQUeM"
      },
      "source": [
        "# SECTION 2: REBUILD METADATA (ONE-TIME SETUP)\n",
        "\n",
        "**Run this section ONLY on first extraction.**\n",
        "\n",
        "These cells create a metadata CSV file by scanning all downloaded 10-K files on disk.\n",
        "Once created, you can skip this section for all future extractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFssIOu3QUeM",
        "outputId": "c631260d-e61b-47ec-a90f-ff33e648746c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Metadata already exists: 83,628 filings\n",
            "   To rebuild, delete datasets/FILINGS_METADATA.csv and re-run this cell\n"
          ]
        }
      ],
      "source": [
        "## üü° Cell 5: Rebuild Metadata from Files on Disk\n",
        "## RUN: FIRST TIME ONLY\n",
        "## SKIP: If datasets/FILINGS_METADATA.csv already exists\n",
        "##\n",
        "## What it does:\n",
        "## - Scans all downloaded 10-K files in RAW_FILINGS/10-K/\n",
        "## - Extracts CIK, year, accession number from filenames\n",
        "## - Creates FILINGS_METADATA.csv with all required columns\n",
        "## - This CSV tells the extractor which files to process\n",
        "##\n",
        "## Expected time: 5-10 minutes for ~80,000 files\n",
        "##\n",
        "## When to re-run:\n",
        "## - First time ever\n",
        "## - If you downloaded NEW 10-K files\n",
        "## - If FILINGS_METADATA.csv is missing or corrupted\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "METADATA_FILE = 'datasets/FILINGS_METADATA.csv'\n",
        "\n",
        "# Check if metadata already exists\n",
        "if os.path.exists(METADATA_FILE):\n",
        "    metadata = pd.read_csv(METADATA_FILE)\n",
        "    print(f\"‚úÖ Metadata already exists: {len(metadata):,} filings\")\n",
        "    print(f\"   To rebuild, delete {METADATA_FILE} and re-run this cell\")\n",
        "else:\n",
        "    print(\"üìä Building metadata from files on disk...\")\n",
        "    print(\"   This may take 5-10 minutes for large datasets\\n\")\n",
        "\n",
        "    from rebuild_metadata_colab import rebuild_for_colab\n",
        "\n",
        "    # Fast mode: extracts CIK, Type, year, accession_number from filenames\n",
        "    rebuild_for_colab(filing_types=['10-K'], fast_mode=True, dry_run=False)\n",
        "\n",
        "    # Add required columns for extraction\n",
        "    metadata = pd.read_csv(METADATA_FILE)\n",
        "\n",
        "    # Add all missing columns with placeholder data\n",
        "    required_columns = {\n",
        "        'Company': lambda: 'Company_' + metadata['CIK'].astype(str),\n",
        "        'Date': lambda: metadata['year'].astype(str) + '-01-01',\n",
        "        'filing_date': lambda: metadata['year'].astype(str) + '-01-01',\n",
        "        'Period of Report': lambda: metadata['year'].astype(str) + '-12-31',\n",
        "        'SIC': lambda: 'Unknown',\n",
        "        'State of Inc': lambda: 'Unknown',\n",
        "        'State location': lambda: 'Unknown',\n",
        "        'Fiscal Year End': lambda: '1231',\n",
        "        'html_index': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
        "        'complete_text_file_link': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
        "        'htm_file_link': lambda: 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=' + metadata['CIK'].astype(str),\n",
        "    }\n",
        "\n",
        "    for col_name, col_value_func in required_columns.items():\n",
        "        if col_name not in metadata.columns:\n",
        "            metadata[col_name] = col_value_func()\n",
        "\n",
        "    metadata.to_csv(METADATA_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n‚úÖ Metadata complete: {len(metadata):,} filings ready for extraction\")\n",
        "    print(f\"   Unique companies: {metadata['CIK'].nunique():,}\")\n",
        "    print(f\"   Years: {sorted(metadata['year'].unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A7eZHIbQUeN"
      },
      "source": [
        "# SECTION 3: CONFIGURE EXTRACTION\n",
        "\n",
        "These cells configure the extraction settings. Run once, then can skip on resume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWaIgr9VQUeN",
        "outputId": "eee0bf6f-6273-4347-a521-fdcd7db19e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Config updated for MD&A extraction\n"
          ]
        }
      ],
      "source": [
        "## üü° Cell 6: Update Config for MD&A Extraction\n",
        "## RUN: FIRST TIME ONLY\n",
        "## SKIP: On resume (config already set)\n",
        "##\n",
        "## What it does:\n",
        "## - Updates config.json to point to your metadata file\n",
        "## - Sets filing type to 10-K only\n",
        "## - Ensures extractor knows where to find filing information\n",
        "##\n",
        "## Expected output: \"‚úÖ Config updated for MD&A extraction\"\n",
        "##\n",
        "## When to re-run:\n",
        "## - First time\n",
        "## - If you pulled new code from repository that changed config.json\n",
        "\n",
        "import json\n",
        "\n",
        "config_path = 'config.json'\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "config['extract_items']['filings_metadata_file'] = 'FILINGS_METADATA.csv'\n",
        "config['extract_items']['filing_types'] = ['10-K']\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Config updated for MD&A extraction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOcNZZeoQUeN",
        "outputId": "3dec985f-1d6d-4269-e14a-eb504058a112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ extract_items.py supports nested directories\n",
            "   Can find files in year-based subdirectories (e.g., 2007/, 2008/)\n"
          ]
        }
      ],
      "source": [
        "## üü° Cell 7: Verify Subdirectory Support\n",
        "## RUN: FIRST TIME ONLY (automatic check)\n",
        "## SKIP: On resume\n",
        "##\n",
        "## What it does:\n",
        "## - Checks if extract_items.py supports year-based subdirectories\n",
        "## - Main branch should already have this fix\n",
        "## - If not found, displays a warning (but likely won't be needed)\n",
        "##\n",
        "## Background:\n",
        "## - Some 10-K files are organized in year folders (e.g., 2007/, 2008/)\n",
        "## - This check ensures the extractor can find files in subdirectories\n",
        "##\n",
        "## Expected output: \"‚úÖ extract_items.py supports nested directories\"\n",
        "\n",
        "file_path = 'extract_items.py'\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Check if already supports subdirectories\n",
        "if 'os.walk(type_dir)' in content:\n",
        "    print(\"‚úÖ extract_items.py supports nested directories\")\n",
        "    print(\"   Can find files in year-based subdirectories (e.g., 2007/, 2008/)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  extract_items.py may not support nested directories\")\n",
        "    print(\"   Main branch should have this fix - try pulling latest code\")\n",
        "    print(\"   If extraction fails with 'FileNotFoundError', update from main branch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKZLAhIwQUeN"
      },
      "source": [
        "# SECTION 4: EXTRACT MD&A\n",
        "\n",
        "**Main extraction cell - Run every time you want to extract or resume.**\n",
        "\n",
        "If encounter error, go to bottom of this notebook for fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM-xr8f4QUeO"
      },
      "outputs": [],
      "source": [
        "## üü¢ Cell 8: Run MD&A Extraction\n",
        "## RUN: EVERY TIME (for first run AND resume)\n",
        "##\n",
        "## What it does:\n",
        "## - Extracts Item 7 (MD&A section) from all 10-K filings\n",
        "## - Automatically SKIPS already-extracted files (resume-friendly!)\n",
        "## - Saves results to datasets/EXTRACTED_FILINGS/10-K/ as JSON files\n",
        "##\n",
        "## Expected behavior:\n",
        "## - First run: Starts from file 1, extracts all\n",
        "## - Resume: Quickly skips already-done files, continues from where it stopped\n",
        "## - Progress bar shows: X/83628 files processed\n",
        "## - Speed: ~10-18 files/second (varies by file size)\n",
        "##\n",
        "## How resume works:\n",
        "## - \"Skip Existing: True\" is set in extraction_configs/mda_only.json\n",
        "## - Script checks if output JSON exists before processing\n",
        "## - Skipped files process in ~0.01 seconds (very fast)\n",
        "## - New extractions take ~5 seconds each (slower)\n",
        "##\n",
        "## Expected time:\n",
        "## - First full run: ~12-24 hours for 83,628 files\n",
        "## - Resume after 50% done: ~6-12 hours\n",
        "##\n",
        "## If disconnected:\n",
        "## 1. Re-run Cells 1-4 (Mount, Navigate, Dependencies, Keep-alive)\n",
        "## 2. Re-run this cell - it will resume automatically!\n",
        "##\n",
        "## Expected output:\n",
        "## - Progress bar moving (even if starting at 0%)\n",
        "## - \"Could not extract\" messages (normal - some files lack MD&A)\n",
        "## - Speed indicator (e.g., 17.93it/s)\n",
        "\n",
        "print(\"üöÄ Starting MD&A extraction...\")\n",
        "print(\"   This will automatically skip already-extracted files\")\n",
        "print(\"   Safe to resume after disconnection!\\n\")\n",
        "\n",
        "!python flexible_extractor.py --config extraction_configs/mda_only.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUUa-xz3QUeO"
      },
      "source": [
        "# SECTION 5: CHECK PROGRESS\n",
        "\n",
        "**Use these cells to monitor extraction progress.**\n",
        "\n",
        "**‚ö†Ô∏è IMPORTANT:** You can ONLY run these cells when Cell 8 (extraction) is NOT running!\n",
        "Colab can only run one cell at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zfw3TJ-VQUeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56ba14b1-59ff-4389-ef60-9abdf9484585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Extraction Progress:\n",
            "   Extracted: 0 files\n",
            "   Expected: 83,628 files\n",
            "   Progress: 0.0%\n",
            "   Remaining: 83,628 files\n"
          ]
        }
      ],
      "source": [
        "## üîµ Cell 9: Check Extraction Progress\n",
        "## RUN: ONLY when extraction cell (Cell 8) is stopped/completed\n",
        "##\n",
        "## What it does:\n",
        "## - Counts how many JSON files have been extracted\n",
        "## - Compares against total expected files from metadata\n",
        "## - Shows percentage complete and files remaining\n",
        "## - Checks sample files for MD&A quality\n",
        "##\n",
        "## When to run:\n",
        "## - BEFORE starting extraction (to see baseline)\n",
        "## - AFTER extraction completes\n",
        "## - AFTER a disconnection (to see how much was done)\n",
        "##\n",
        "## ‚ö†Ô∏è DO NOT run while Cell 8 is running! (Colab limitation)\n",
        "##\n",
        "## Expected output:\n",
        "## - Extracted: X files\n",
        "## - Expected: 83,628 files (or your total)\n",
        "## - Progress: X%\n",
        "## - Sample quality check showing 3 random files\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
        "\n",
        "if os.path.exists(extracted_dir):\n",
        "    # Count all JSON files (including in subdirectories if any)\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk(extracted_dir):\n",
        "        all_files.extend([os.path.join(root, f) for f in files if f.endswith('.json')])\n",
        "\n",
        "    # Get expected total from metadata\n",
        "    metadata = pd.read_csv('datasets/FILINGS_METADATA.csv')\n",
        "    expected = len(metadata[metadata['Type'] == '10-K'])\n",
        "\n",
        "    print(f\"üìä Extraction Progress:\")\n",
        "    print(f\"   Extracted: {len(all_files):,} files\")\n",
        "    print(f\"   Expected: {expected:,} files\")\n",
        "    print(f\"   Progress: {len(all_files)/expected*100:.1f}%\")\n",
        "    print(f\"   Remaining: {expected - len(all_files):,} files\")\n",
        "\n",
        "    # Check sample files for MD&A content\n",
        "    if len(all_files) > 0:\n",
        "        print(f\"\\nüìã Sample Quality Check (first 3 files):\")\n",
        "        for fpath in all_files[:3]:\n",
        "            fname = os.path.basename(fpath)\n",
        "            try:\n",
        "                with open(fpath, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "                    has_mda = 'item_7' in data and len(data.get('item_7', '')) > 100\n",
        "                    mda_len = len(data.get('item_7', ''))\n",
        "                    print(f\"   {fname}: {'‚úÖ' if has_mda else '‚ùå'} MD&A ({mda_len:,} chars)\")\n",
        "            except Exception as e:\n",
        "                print(f\"   {fname}: ‚ö†Ô∏è Error reading file - {e}\")\n",
        "else:\n",
        "    print(\"‚ùå No extraction directory found\")\n",
        "    print(f\"   Expected location: {extracted_dir}\")\n",
        "    print(\"   Run Cell 8 to start extraction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdbDHNIqQUeO"
      },
      "source": [
        "# SECTION 6: CREATE ANALYSIS FILES\n",
        "\n",
        "**Run this AFTER extraction is complete to create analysis-ready files.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfrNaHlkQUeP"
      },
      "outputs": [],
      "source": [
        "## üîµ Cell 10: Create Analysis Files (CSV + Parquet)\n",
        "## RUN: AFTER extraction is complete (or mostly complete)\n",
        "##\n",
        "## What it does:\n",
        "## - Reads all extracted JSON files\n",
        "## - Creates TWO output files:\n",
        "##   1. mda_metadata.csv (lightweight, just metadata + stats)\n",
        "##   2. mda_full.parquet (full MD&A text, compressed, ~500MB-1GB)\n",
        "##\n",
        "## Output files:\n",
        "## - mda_metadata.csv: Filename, CIK, company, dates, MD&A length, etc.\n",
        "## - mda_full.parquet: Full MD&A text + metadata (for text analysis)\n",
        "##\n",
        "## Where saved:\n",
        "## - /content/drive/MyDrive/EDGAR_Project/ (your project root)\n",
        "##\n",
        "## When to run:\n",
        "## - After all extraction is done\n",
        "## - Can run on partial extractions (will process whatever exists)\n",
        "##\n",
        "## Expected time: 5-15 minutes for 80,000+ files\n",
        "##\n",
        "## Uses:\n",
        "## - CSV: Quick viewing in Excel/Google Sheets\n",
        "## - Parquet: Fast loading in Python/R for text analysis\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"üìä Creating analysis files...\")\n",
        "print(\"   This may take 5-15 minutes depending on file count\\n\")\n",
        "\n",
        "extracted_dir = 'datasets/EXTRACTED_FILINGS/10-K'\n",
        "metadata_records = []\n",
        "full_data_records = []\n",
        "\n",
        "# Process all extracted JSON files\n",
        "json_files = []\n",
        "for root, dirs, files in os.walk(extracted_dir):\n",
        "    json_files.extend([os.path.join(root, f) for f in files if f.endswith('.json')])\n",
        "\n",
        "for filepath in tqdm(json_files, desc=\"Processing\"):\n",
        "    filename = os.path.basename(filepath)\n",
        "\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            filing = json.load(f)\n",
        "\n",
        "        # Metadata (lightweight)\n",
        "        metadata_records.append({\n",
        "            'filename': filename,\n",
        "            'cik': filing.get('cik', ''),\n",
        "            'company': filing.get('company', ''),\n",
        "            'filing_date': filing.get('filing_date', ''),\n",
        "            'period_of_report': filing.get('period_of_report', ''),\n",
        "            'year': filing.get('period_of_report', '')[:4] if filing.get('period_of_report', '') else '',\n",
        "            'has_mda': 'item_7' in filing and len(filing.get('item_7', '')) > 0,\n",
        "            'mda_length': len(filing.get('item_7', '')),\n",
        "            'json_path': filepath\n",
        "        })\n",
        "\n",
        "        # Full data (with MD&A text)\n",
        "        if 'item_7' in filing and filing['item_7']:\n",
        "            full_data_records.append({\n",
        "                'cik': filing.get('cik', ''),\n",
        "                'company': filing.get('company', ''),\n",
        "                'filing_date': filing.get('filing_date', ''),\n",
        "                'period_of_report': filing.get('period_of_report', ''),\n",
        "                'year': filing.get('period_of_report', '')[:4] if filing.get('period_of_report', '') else '',\n",
        "                'mda_text': filing.get('item_7', '')\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error processing {filename}: {e}\")\n",
        "\n",
        "# Save files\n",
        "df_meta = pd.DataFrame(metadata_records)\n",
        "df_full = pd.DataFrame(full_data_records)\n",
        "\n",
        "meta_path = '/content/drive/MyDrive/EDGAR_Project/mda_metadata.csv'\n",
        "parquet_path = '/content/drive/MyDrive/EDGAR_Project/mda_full.parquet'\n",
        "\n",
        "df_meta.to_csv(meta_path, index=False)\n",
        "df_full.to_parquet(parquet_path, compression='gzip', index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Files created:\")\n",
        "print(f\"   üìÑ Metadata CSV: {len(df_meta):,} records ({os.path.getsize(meta_path)/1024:.1f} KB)\")\n",
        "print(f\"   üì¶ Parquet: {len(df_full):,} records ({os.path.getsize(parquet_path)/(1024**2):.1f} MB)\")\n",
        "print(f\"\\nüìä Statistics:\")\n",
        "print(f\"   Files with MD&A: {df_meta['has_mda'].sum():,}\")\n",
        "print(f\"   Files without MD&A: {(~df_meta['has_mda']).sum():,}\")\n",
        "if len(df_full) > 0:\n",
        "    print(f\"   Years covered: {sorted(df_full['year'].unique())}\")\n",
        "    print(f\"   Avg MD&A length: {df_full['mda_text'].str.len().mean():,.0f} chars\")\n",
        "print(f\"\\nüéâ Ready for analysis!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q7Kpy0yQUeP"
      },
      "source": [
        "# SECTION 7: QUICK RESUME (AFTER DISCONNECTION)\n",
        "\n",
        "**If Colab disconnects, run ONLY these cells to resume:**\n",
        "\n",
        "1. Cell 1 (Mount Drive)\n",
        "2. Cell 2 (Navigate to repo)\n",
        "3. Cell 3 (Install dependencies) - ONLY if runtime restarted\n",
        "4. Cell 4 (Keep-alive)\n",
        "5. Cell 8 (Resume extraction)\n",
        "\n",
        "**Skip Cells 5-7** (metadata rebuild and config - already done!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG2Htj5eQUeP"
      },
      "outputs": [],
      "source": [
        "## üîµ Cell 11: Quick Resume Helper\n",
        "## RUN: OPTIONAL - Only if you want a one-click resume\n",
        "##\n",
        "## What it does:\n",
        "## - Combines all resume steps into one cell\n",
        "## - Checks Drive connection\n",
        "## - Navigates to repo\n",
        "## - Resumes extraction\n",
        "##\n",
        "## When to use:\n",
        "## - After a disconnection\n",
        "## - If you already ran dependencies (Cell 3) and they're still installed\n",
        "##\n",
        "## ‚ö†Ô∏è If this fails with import errors, run Cell 3 first!\n",
        "\n",
        "import os\n",
        "\n",
        "# Check Drive\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    print(\"‚ùå Drive not mounted! Run Cell 1 first.\")\n",
        "else:\n",
        "    # Navigate to repo\n",
        "    os.chdir('/content/drive/MyDrive/EDGAR_Project/edgar-crawler')\n",
        "    print(f\"‚úÖ Ready to resume from: {os.getcwd()}\")\n",
        "    print(\"\\nüîÑ Resuming extraction...\\n\")\n",
        "\n",
        "    # Resume extraction\n",
        "    !python flexible_extractor.py --config extraction_configs/mda_only.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Fix: Remount Drive & Resume\n",
        "What Happened:\n",
        "- Extraction started checking files (4 minutes, 6 seconds in)\n",
        "- Tried to write: 1212545_10K_2001_0000950123-11-022298.json\n",
        "- Drive connection failed completely (Errno 5 = I/O error)\n",
        "- Process crashed"
      ],
      "metadata": {
        "id": "YGrxImq9xKV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Force Remount Drive\n",
        "# Force reconnect to Drive\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ere2O-azxKme",
        "outputId": "37c85e30-e7fa-441f-ff07-6af2e2482190"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Verify Drive Works\n",
        "import os\n",
        "\n",
        "# Test write capability\n",
        "test_dir = '/content/drive/MyDrive/EDGAR_Project/edgar-crawler/datasets/EXTRACTED_FILINGS/10-K'\n",
        "\n",
        "try:\n",
        "    # Try to create a test file\n",
        "    test_file = os.path.join(test_dir, '_test_connection.json')\n",
        "    with open(test_file, 'w') as f:\n",
        "        f.write('{\"test\": \"connection\"}')\n",
        "    os.remove(test_file)\n",
        "    print(\"‚úÖ Drive is working and writable\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Drive still has issues: {e}\")\n",
        "    print(\"You may need to restart the Colab runtime\")\n"
      ],
      "metadata": {
        "id": "91czBovaxKp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9e6804-c5fa-4244-edd0-a95da7da97c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Drive still has issues: [Errno 5] Input/output error: '/content/drive/MyDrive/EDGAR_Project/edgar-crawler/datasets/EXTRACTED_FILINGS/10-K/_test_connection.json'\n",
            "You may need to restart the Colab runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate Back to Repo\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/EDGAR_Project/edgar-crawler')\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3IDyHJ4xKsz",
        "outputId": "d32386ea-6e7c-4dac-fd28-d56748547007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Working directory: /content/drive/MyDrive/EDGAR_Project/edgar-crawler\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Resume Extraction\n",
        "!python flexible_extractor.py --config extraction_configs/mda_only.json\n"
      ],
      "metadata": {
        "id": "TmkbpYDyxKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ew30t0NxK1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL1qOt-rxK3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNtTubSoxK4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}